# -*- coding: utf-8 -*-
"""conversational_chain_final_26_11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_2_Ae4w6lypt3nnC85AddVc3CRuGLhK5
"""

pip install llama-cpp-python rank_bm25 qdrant-client reportlab qdrant-client langchain huggingface-hub PyPDF2 tiktoken faiss-cpu rank_bm25 faiss-gpu pandas reportlab google-cloud-aiplatform sentence_transformers chromadb

# -*- coding: utf-8 -*-
"""Retrival_QA_v2_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E4Mggb_QKHteGPX_vbrTPiW7ffQ60Dql

This is a successor of Retrival_QA_v1 stored in the Drive, here we will be changing the vector store to FAISS
"""

import sys
import os

from langchain.llms import HuggingFaceTextGenInference
from langchain.callbacks import StreamingStdOutCallbackHandler
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from langchain.llms import HuggingFaceTextGenInference
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.rl_config import defaultPageSize
from reportlab.lib.units import inch
from PyPDF2 import PdfReader
from langchain.vectorstores import Qdrant
from langchain.llms import LlamaCpp
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain import PromptTemplate, HuggingFaceHub, LLMChain
from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.embeddings import VertexAIEmbeddings
from langchain.vectorstores import Chroma
from huggingface_hub import login
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.vectorstores import FAISS
from langchain.schema import prompt
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.vectorstores import FAISS
os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES']='0'
from langchain.llms import LlamaCpp, HuggingFaceTextGenInference

import textwrap
import pandas as pd

# Function to create a PDF file with a list
def create_pdf_with_list(file_name, my_list):
    doc = SimpleDocTemplate(file_name, pagesize=letter)
    styles = getSampleStyleSheet()
    story = []

    # Add a title
    title = "Business requirement Document and the Associated Test cases"
    story.append(Paragraph(title, styles["Title"]))

    # Add a spacer
    story.append(Spacer(1, 12))

    # Add the list items as paragraphs
    for item in my_list:
        paragraph = Paragraph(item, styles["Normal"])
        story.append(paragraph)
        story.append(Spacer(1, 12))  # Add some space between items

    # Build the PDF document
    doc.build(story)



# File name for the PDF
file_name = "data_for_RQA.pdf"



#login(token="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")



df = pd.read_csv("/content/Finacle_data_v2.0.csv")

my_dict = {}
text_col =[]
my_dict['text'] = []

for _, row in df.iterrows():
  prompt= "Below is an input query that the user would provide and the associated asnwers is the Response that you would need to provide back to the user.\n\n"
  instruction=str(row['instruction'])
  input_query = str(row['input'])
  response = str(row['output'])
  if len(input_query.strip())== 0:
    text =  prompt + "instruction:\n" + instruction + "\nResponse:\n" +response
  else:
    #text = ("Instruction:\n" + instruction +"\nInput Query:\n" + input_query+"\nResponse:\n" +response)
    text = ("\nInput Query:\n" + input_query+ "Test cases for"+input_query + "\nResponse:\n" +response)
    text_col.append(text)
    my_dict['text'].append(text)
df.loc[:, "text"]= text_col

#Writing the text_list as a pdf:
my_list = text_col
# Create the PDF with the list
create_pdf_with_list(file_name, my_list)
print(f"PDF file '{file_name}' has been created.")

#Reading the PDF

doc_reader = PdfReader('/content/data_for_RQA.pdf')

#read data from the file and put them into a variable called raw_text

raw_text = ''
for i, page in enumerate (doc_reader.pages):
  text = page.extract_text()
  if text:
    raw_text +=text

#Splitting up the text into smaller chunks for indexing
text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size = 4000,
    chunk_overlap  = 600,
    length_function = len,
)

texts = text_splitter.split_text(raw_text)

"""**Embedding model ________________**"""

model_name = "BAAI/bge-large-en-v1.5" # change to lama embedding
encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity

model_norm = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs={'device': 'cuda'},
    encode_kwargs=encode_kwargs
)

def wrap_text_preserve_newlines(text, width=110):
    # Split the input text into lines based on newline characters
    lines = text.split('\n')

    # Wrap each line individually
    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]

    # Join the wrapped lines back together using newline characters
    wrapped_text = '\n'.join(wrapped_lines)

    return wrapped_text

def process_llm_response(llm_response):
    print(wrap_text_preserve_newlines(llm_response['result']))
    #print('\n\nSources:')
    for source in llm_response["source_documents"]:
        print(source.metadata['source'])

from langchain.llms import HuggingFaceTextGenInference

pip install text_generation

"""**LLM change -------**"""

#inference_server_url_local = "http://20.31.104:80"
#inference_server_url_local = "http://172.205.67:80"

llm_local = HuggingFaceTextGenInference(
    inference_server_url=inference_server_url_local,
    max_new_tokens=300,
    top_k=10,
    top_p=0.95,
    typical_p=0.95,
    temperature=0.01,
    repetition_penalty=1.03,
            streaming = True,
callbacks=[StreamingStdOutCallbackHandler()]
)

#llm_lama2_7b = LlamaCpp(model_path='/home/asgadmin/llm-project/BRD-7b_no_venv/gguf/Llama-2-13B-chat-GGUF/',max_tokens=4096, n_predict =4096, n_ctx=4096)

#instead of chroma we are planning to use the Async vector db, AWAIT
#instead of chroma we will be going with faiss
# Embed and store the texts
# Supplying a persist_directory will store the embeddings on disk

persist_directory = 'db'

## Here is the nmew embeddings being used
embedding = model_norm

#vectordb = Chroma.from_texts(texts=texts, embedding=embedding, persist_directory=persist_directory)

vectordb = FAISS.from_texts(texts=texts, embedding=embedding)

vectordb.save_local("faiss_index")

new_db = FAISS.load_local("faiss_index", embedding)

##Tring to create a Ensemble retrivers

#retriever = vectordb.as_retriever(search_kwargs={"k": 5})

#bm25_retriever = BM25Retriever.from_texts(texts)
#bm25_retriever.k = 3

#vectordb = FAISS.from_texts(texts=texts, embedding=embedding)
faiss_retriever = new_db.as_retriever(search_kwargs={"k": 3})

#intializing the ensembel retriver

# initialize the ensemble retriever
#ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])

from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationEntityMemory, ConversationKGMemory, ConversationSummaryMemory
from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE


buffer_memory = ConversationBufferMemory( memory_key = 'chat_history', return_messages=True)

Buffer_window_memory = ConversationBufferWindowMemory(memory_key="chat_history", k=4,
                                  return_messages=True ) #returns the chat messages as a list instead of a single string
entity_memory = ConversationEntityMemory(llm=llm_local, memory_key="chat_history",
                                  return_messages=True ) #returns the chat messages as a list instead of a single string
kg_memory = memory = ConversationKGMemory(llm=llm_local, memory_key='chat_history',
                                          retrun_messages=True)
summary_memory = ConversationSummaryMemory(llm=llm_local, memory_key='chat_history',
                                          retrun_messages=True)

from langchain.chains.question_answering import load_qa_chain
from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT

custom_template  = """ <<SYS>> You are a standalone question generator, Given the following Chat History and a follow up Input generate a standalone question\n
The question is to the point.\n
<</SYS>>

Chat History:
{chat_history}

Follow Up Input: {question}

Standalone question:"""

from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

general_system_template = """
<<SYS>> You are a test case generator, based on the given context and question you generate the most relavent test cases from the context and you do not deviate from the given task. You are a responsible AI, that will not answer any question apart from test case generation. You will restructure, rephrase, and alter the output based on the users input. You always provide the test cases in the fromat "Condition 1:\n
Test cases 1:/n
Expected Results: /n".
The test cases and conditions will be sequentially numbered and you include the informations from the provided context and will not add up informations.
 ----
{context}
----
<</SYS>"""
general_user_template = "Question:```{question}``` " if "{question}" in general_user_template else ""
messages = [
            SystemMessagePromptTemplate.from_template(general_system_template),
            HumanMessagePromptTemplate.from_template(general_user_template)
]
qa_prompt = ChatPromptTemplate.from_messages( messages )

CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)

doc_chain = load_qa_chain(llm_local, chain_type="stuff", prompt=qa_prompt) #other types "stuff", "map_reduce", "refine", or "map_rerank"

question_generator = LLMChain(llm=llm_local, prompt = CUSTOM_QUESTION_PROMPT)

no_doc = "I am sorry I am not able to generate any test case for the requirement. I am not familier on this part and I am continously learning and evolving. I am sorry for the convinence"

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
from langchain.chains import ConversationalRetrievalChain
# Initialize the ConversationalRetrievalChain with the question generator
chat_chain = ConversationalRetrievalChain(#llm_local,
                                          retriever=faiss_retriever,
                                          question_generator=question_generator,
                                          combine_docs_chain=doc_chain,
                                          memory = buffer_memory,
                                          rephrase_question = False,
                                          output_key = 'answer',
                                          response_if_no_docs_found = no_doc,
                                          return_generated_question = False,
                                          return_source_documents = False,
                                          )

#combine_docs_ptompt=""
#condense_question_prompt = CUSTOM_QUESTION_PROMPT

query = """Please provide the conditions and test cases for Verification of Issued inventory"""
test_query_restructure = """Test case 1: Check if the system allows the user to navigate to inventory movement between locations screen on entering HIMC
Expected Result: System should allow the user to navigate to inventory movement between locations screen on entering HIMC

Test case 3: Check if the system allows the user to enter the transaction id in "inventory transaction id" field
Expected Result: System should not allow the user to enter the transaction id in "inventory transaction id" field NONE

restructure the above output like the below

testcase_1
Onward Expected results"""
test_query = """Please provide 2 test cases for Verification of Issued inventory along with the expected results"""
query3 = """generate test cases for Capture outward remittance in LCY debit to FCY"""

query = "Generate testcases for Generation of  MT740 message for Reimbursement Bank in Finacle application of core banking"

result['answer'] = chat_chain({"question": query})

result['answer']

result['question']

#############cosmos

pip install azure-cosmos

from azure.cosmos import CosmosClient, PartitionKey, exceptions

pip install azure-cosmos

url = "https://xbot.documents.azure.com:443/"
key = 'YaBgRfPa2jFq5DxeT10iy99u9DoBSqAgZOEHMH2RekT9ilaC97orQnBlN2MS4ncTx4SEs8ngh6OHACDbFE7d3A=='
client = CosmosClient(url, credential=key)

from langchain.memory import ConversationBufferMemory
from langchain.schema import BaseMemory
from azure.cosmos import CosmosClient

class CustomMemory(BaseMemory):
    def __init__(self, buffer_size, endpoint, key, database_name, container_name, partition_key, return_messages=False):
        self.buffer_size = buffer_size
        self.buffer_memory = ConversationBufferMemory(return_messages=True)
        self.cosmos_memory = CosmosMemory(endpoint, key, database_name, container_name, partition_key, return_messages=return_messages)

    def save_context(self, inputs, outputs):
        # Save the conversation context to the buffer memory
        self.buffer_memory.save_context(inputs, outputs)

        # Trim the buffer memory to the buffer size
        if len(self.buffer_memory.buffer_as_messages) > self.buffer_size:
            self.buffer_memory.buffer_as_messages = self.buffer_memory.buffer_as_messages[-self.buffer_size:]

        # Save the conversation context to the Cosmos memory
        self.cosmos_memory.save_context(inputs, outputs)

    def load_memory_variables(self, inputs):
        # Load the memory variables from the buffer memory
        buffer_memory_variables = self.buffer_memory.load_memory_variables(inputs)

        # Load the memory variables from the Cosmos memory
        cosmos_memory_variables = self.cosmos_memory.load_memory_variables(inputs)

        # Combine the memory variables from both memories
        combined_memory_variables = {}
        for key in buffer_memory_variables:
            combined_memory_variables[key] = buffer_memory_variables[key] + cosmos_memory_variables[key]

        return combined_memory_variables

    def clear(self):
        # Clear the buffer memory
        self.buffer_memory.clear()

        # Clear the Cosmos memory
        self.cosmos_memory.clear()


class CosmosMemory:
    def __init__(self, endpoint, key, database_name, container_name, partition_key, return_messages=False):
        self.client = CosmosClient(endpoint, key)
        self.database_name = database_name
        self.container_name = container_name
        self.partition_key = partition_key
        self.return_messages = return_messages

    def save_context(self, input_data, output_data):
        database = self.client.get_database_client(self.database_name)
        container = database.get_container_client(self.container_name)
        container.create_item({
            "input": input_data,
            "output": output_data
        })

    def load_memory_variables(self, input_data):
        database = self.client.get_database_client(self.database_name)
        container = database.get_container_client(self.container_name)
        query = f"SELECT * FROM c WHERE c.input = '{input_data}'"
        items = container.query_items(query, enable_cross_partition_query=True)
        result = []
        for item in items:
            if self.return_messages:
                result.append((item["input"], item["output"]))
            else:
                result.append(item["output"])
        return {"history": result}

    def clear(self):
        database = self.client.get_database_client(self.database_name)
        container = database.get_container_client(self.container_name)
        container.delete_items(
            query=f"SELECT * FROM c WHERE c.partitionKey = '{self.partition_key}'",
            enable_cross_partition_query=True
        )

# Initialize the CosmosMemory with the necessary parameters
cosmos_memory = CosmosMemory(
    endpoint="https://xbot.documents.azure.com:443/",
    key="YaBgRfPa2jFq5DxeT10iy99u9DoBSqAgZOEHMH2RekT9ilaC97orQnBlN2MS4ncTx4SEs8ngh6OHACDbFE7d3A==",
    database_name="xbot1",
    container_name="conversational_chain",
    partition_key="your_partition_key",
    return_messages=True
)

# Initialize the ConversationalRetrievalChain with the CosmosMemory
chat_chain = ConversationalRetrievalChain(
    retriever=faiss_retriever,
    question_generator=question_generator,
    combine_docs_chain=doc_chain,
    memory=CustomMemory(buffer_size=100, endpoint=cosmos_memory.endpoint, key=cosmos_memory.key, database_name=cosmos_memory.database_name, container_name=cosmos_memory.container_name, partition_key=cosmos_memory.partition_key, return_messages=True),
    rephrase_question=False,
    output_key='answer',
    response_if_no_docs_found=no_doc,
    return_generated_question=False
)

import uuid

# Generate a random UUID (version 4)
udid = str(uuid.uuid4())

print("Generated UDID:", udid)

result.keys()

result['answer']

buffer_memory.load_memory_variables({})

buffer_memory.load_memory_variables({})

#qa__ = ConversationalRetrievalChain.from_llm(llm_local, faiss_retriever)

#chat_history = []
#query = "list the test cases anlong with the conditions and expected results for Verification of Issued inventory"
#result = chat_chain({"question": query, "chat_history": chat_history})

vectordbkwargs = {"search_distance": 0.9}

chat_history = []
query = "list the test cases anlong with the conditions and expected results for Verification of Issued inventory"
result = chat_chain({"question": query, "chat_history": chat_history, "vectordbkwargs": vectordbkwargs})

chat_history = [(query, result["answer"])]

query = """resurtucture the output as below
condtion one:
testcase_1
Expected results"""
result = chat_chain({"question": query, "chat_history": chat_history})

pip install text_generation llama-cpp-python rank_bm25 qdrant-client reportlab qdrant-client langchain huggingface-hub PyPDF2 tiktoken faiss-cpu rank_bm25 faiss-gpu pandas reportlab google-cloud-aiplatform sentence_transformers chromadb

pip install text_generation

buffer_memory = ConversationBufferMemory( memory_key = 'chat_history', return_messages=True)

import sys
import os
from langchain.chains.question_answering import load_qa_chain
from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT
from langchain.chains.question_answering import load_qa_chain
from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT
from langchain.llms import HuggingFaceTextGenInference
from langchain.callbacks import StreamingStdOutCallbackHandler
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.llms import HuggingFaceTextGenInference
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.rl_config import defaultPageSize
from reportlab.lib.units import inch
from PyPDF2 import PdfReader
from langchain.vectorstores import Qdrant
from langchain.llms import LlamaCpp
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain import PromptTemplate, HuggingFaceHub, LLMChain
from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.embeddings import VertexAIEmbeddings
from langchain.vectorstores import Chroma
from huggingface_hub import login
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.vectorstores import FAISS
from langchain.schema import prompt
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.vectorstores import FAISS
os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES']='0'
from langchain.llms import LlamaCpp, HuggingFaceTextGenInference
from langchain.llms import HuggingFaceTextGenInference

import textwrap
import pandas as pd

inference_server_url_local = "http://20.31.133.104:80"
#inference_server_url_local = "http://172.205.176.67:80"
llm_local = HuggingFaceTextGenInference(
    inference_server_url=inference_server_url_local,
    max_new_tokens=500,
    top_k=10,
    top_p=0.95,
    typical_p=0.95,
    temperature=0.01,
    repetition_penalty=1.03,
            streaming = True,
callbacks=[StreamingStdOutCallbackHandler()]
)

model_name = "BAAI/bge-large-en-v1.5" # change to lama embedding
encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity

model_norm = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs={'device': 'cuda'},
    encode_kwargs=encode_kwargs
)

embedding = model_norm
new_db = FAISS.load_local("/content/embeddings", embedding)

#vectordb = FAISS.from_texts(texts=texts, embedding=embedding)
faiss_retriever = new_db.as_retriever(search_kwargs={"k": 3})


custom_template  = """ <<SYS>> Consilidate the Chat history and the question to a single stand alone question. <</SYS>>
Chat History:
{chat_history}

Follow Up Input: {question}

Standalone question:"""

general_system_template = """
You are a test case generator, based on the given context and question you generate the test cases and you do not deviate from the given task. You are a responsible AI, that will not answer any question apart from test case generation. You will restructure, rephrase, and alter the output based on the users input. You always provide the test cases in the fromat "Condition -> Test cases -> Expected Results". The test cases and conditions will be sequentially numbered and you include the informations from the provided context and will not add up informations.
 ----
{context}

----
"""
general_user_template = "Question:```{question}```"
messages = [
            SystemMessagePromptTemplate.from_template(general_system_template),
            HumanMessagePromptTemplate.from_template(general_user_template)
]
qa_prompt = ChatPromptTemplate.from_messages( messages )

CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)

doc_chain = load_qa_chain(llm_local, chain_type="stuff", prompt=qa_prompt) #other types "stuff", "map_reduce", "refine", or "map_rerank"

question_generator = LLMChain(llm=llm_local, prompt = CUSTOM_QUESTION_PROMPT, verbose=False)

no_doc = "I am sorry I am not able to generate any test case for the requirement. I am not familier on this part and I am continously learning and evolving. I am sorry for the convinence"

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
from langchain.chains import ConversationalRetrievalChain
# Initialize the ConversationalRetrievalChain with the question generator

chat_chain = ConversationalRetrievalChain(#llm_local,
                                          retriever=faiss_retriever,
                                          question_generator=question_generator,
                                          combine_docs_chain=doc_chain,
                                          memory = buffer_memory,
                                          rephrase_question = False,
                                          output_key = 'answer',
                                          response_if_no_docs_found = no_doc,
                                          return_generated_question = True,
                                          return_source_documents = False,
                                          verbose = False
                                          )
query = "1 test case for verification of issued inventory "

question = "[INST]" + query + "[/INST]"

result = chat_chain({"question": question})

result['answer']

print(buffer_memory.buffer)

